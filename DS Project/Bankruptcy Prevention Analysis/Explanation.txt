Good Morning everyone, my name is Nikhil & I'm going to explain the EDA part.

The dataset is provided in an Excel file named Bankruptcy.xlsx.
The features industrial_risk, management_risk, financial_flexibility, credibility, competitiveness, and operating_risk have discrete values (0, 0.5, 1).

As we can se There are no missing values in the dataset.

Now we will move towards The visualization part.
The set of histograms represents the distributions of the six features: industrial_risk, management_risk, financial_flexibility, credibility, competitiveness, and operating_risk.

Industrial Risk:
The feature industrial_risk has three distinct values: 0, 0.5, and 1, corresponding to low, medium, and high risk, respectively.
The distribution shows that companies with high industrial risk (1) are the most frequent, followed by low risk (0) and medium risk (0.5).

Management Risk
Similar to industrial risk, management_risk has three values.
The distribution shows that companies with high management risk (1) are the most frequent, followed by low risk (0) and medium risk (0.5).

Financial Flexibility:
The feature financial_flexibility also has three values.
The distribution shows that companies with low financial flexibility (0) are the most frequent, followed by high flexibility (1) and medium flexibility (0.5).

Credibility:
The feature credibility has three values.
The distribution shows that companies with high credibility (1) are the most frequent, followed by low credibility (0) and medium credibility (0.5).

Competitiveness:
The feature competitiveness has three values.
The distribution shows that companies with low competitiveness (0) are the most frequent, followed by high competitiveness (1) and medium competitiveness (0.5).

Operating Risk:
The feature operating_risk has three values.
The distribution shows that companies with low operating risk (0) are the most frequent, followed by high risk (1) and medium risk (0.5).







The bar chart represents the distribution of the target variable, "class," which indicates whether a company is bankrupt or not.

Insight:
The chart shows that there are more companies labeled as "non-bankruptcy" compared to those labeled as "bankruptcy."
This indicates a class imbalance, with "non-bankruptcy" being the majority class.



General Insights from EDA:

Each feature has discrete values, indicating different levels of risk, flexibility, credibility, and competitiveness.
There are imbalances in the feature distributions, with some levels (e.g., high risk, low flexibility) being more common than others.
The class distribution imbalance needs to be considered during model building to avoid biased predictions.



These insights from the EDA phase helps us to  understand the characteristics of the data and prepare for the subsequent steps in the project.








Steps in Model Building:

1. Encoding the Target Variable:

The target variable, class, is initially categorical with values 'bankruptcy' and 'non-bankruptcy'.
We encode this variable to numerical values for the model to process:
'non-bankruptcy' is encoded as 0.
'bankruptcy' is encoded as 1.


2. Splitting the Data:

The dataset is split into features (X) and target (y).
The features (X) include all columns except the target variable (class).
The target (y) is the encoded class variable.
We split the data into training and testing sets to evaluate the model's performance on unseen data.
We use an 80-20 split, meaning 80% of the data is used for training and 20% for testing.


3. Training the Model:

We use logistic regression, a simple and effective algorithm for binary classification.
The model is trained on the training data (X_train and y_train).


4. Saving the Model:

After training the model, we save it to a file using joblib.
This allows us to load and use the model later without retraining it.




Explanation of Model Building Steps:

--> Encoding the Target Variable: This step transforms the target variable from categorical to numerical, making it suitable for machine learning algorithms. This encoding helps the logistic regression model understand and differentiate between the two classes.

--> Splitting the Data: By splitting the dataset into training and testing sets, we ensure that we can evaluate the model's performance on data it hasn't seen before. This helps prevent overfitting, where the model performs well on the training data but poorly on new data.

--> Training the Model: Logistic regression is used because it is a straightforward and interpretable algorithm for binary classification. The training process involves finding the best-fitting model that can predict the target variable based on the features.


--> Saving the Model: Saving the trained model allows us to deploy it for real-time predictions. This step ensures that the model can be reused without retraining, saving time and computational resources.


Expected Outcome:
By the end of this phase, we have a trained logistic regression model that is saved and ready for evaluation. The model can now be tested on the testing set to determine its accuracy and other performance metrics.